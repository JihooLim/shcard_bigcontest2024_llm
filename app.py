import os
import numpy as np
import pandas as pd

from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm
import faiss

import streamlit as st
#ê°€ë‚˜ë¼
# ê²½ë¡œ ì„¤ì •
data_path = './data'
module_path = './modules'

# Gemini ì„¤ì •
import google.generativeai as genai

GOOGLE_API_KEY = st.secrets["API_KEY"]

genai.configure(api_key=GOOGLE_API_KEY)

# Gemini ëª¨ë¸ ì„ íƒ
model = genai.GenerativeModel("gemini-1.5-flash")


# CSV íŒŒì¼ ë¡œë“œ
## ìì²´ ì „ì²˜ë¦¬ë¥¼ ê±°ì¹œ ë°ì´í„° íŒŒì¼ í™œìš©
csv_file_path = "JEJU_DATA.csv"
df = pd.read_csv(os.path.join(data_path, csv_file_path),encoding = 'cp949')

# ìµœì‹ ì—°ì›” ë°ì´í„°ë§Œ ê°€ì ¸ì˜´
df = df[df['ê¸°ì¤€ì—°ì›”'] == df['ê¸°ì¤€ì—°ì›”'].max()].reset_index(drop=True)


# Streamlit App UI

st.set_page_config(page_title="ğŸŠì œì£¼ë„ ë§›ì§‘ ì¶”ì²œ")



# Replicate Credentials
with st.sidebar:
    st.title("**ğŸŠì œì£¼ë„ ë§›ì§‘ ì¶”ì²œ**")

    st.write("")
    st.markdown("""
        <style>
        .sidebar-text {
        color: #B271FF;
        font-size: 20px;
        font-weight: bold;
        }
     </style>
     """, unsafe_allow_html=True)

    st.sidebar.markdown('<p class="sidebar-text">ì‹œê°„ëŒ€ê°€ ì–´ë–»ê²Œ ë˜ì‹œë‚˜ìš”??</p>', unsafe_allow_html=True)

    # selectbox ë ˆì´ë¸” ê³µë°± ì œê±°
    st.markdown(
        """
        <style>
        .stSelectbox label {  /* This targets the label element for selectbox */
            display: none;  /* Hides the label element */
        }
        .stSelectbox div[role='combobox'] {
            margin-top: -20px; /* Adjusts the margin if needed */
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    time = st.sidebar.selectbox("", ["ìƒê´€ ì—†ìŒ","ì•„ì¹¨", "ì ì‹¬", "ì˜¤í›„", "ì €ë…", "ë°¤"], key="time")

    st.write("")

    st.sidebar.markdown('<p class="sidebar-text">í¬ë§ ê°€ê²©ëŒ€ëŠ” ì–´ë–»ê²Œ ë˜ì‹œë‚˜ìš”??</p>', unsafe_allow_html=True)

    # radio ë ˆì´ë¸” ê³µë°± ì œê±°
    st.markdown(
        """
        <style>
        .stSelectbox label {  /* This targets the label element for selectbox */
            display: none;  /* Hides the label element */
        }
        .stSelectbox div[role='combobox'] {
            margin-top: -20px; /* Adjusts the margin if needed */
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    price = st.sidebar.selectbox("", ['ìƒê´€ ì—†ìŒ','ìµœê³ ê°€', 'ê³ ê°€', 'í‰ê·  ê°€ê²©ëŒ€', 'ì¤‘ì €ê°€', 'ì €ê°€'], key="price")
    

    st.markdown(
        """
         <style>
         [data-testid="stSidebar"] {
         background-color: #ff9900;
         }
        </style>
        """, unsafe_allow_html=True)
    st.write("")

st.title("ì–´ì„œ ì™€ìš©!ğŸ‘‹")
st.subheader("ì¸ê¸°ìˆëŠ” :orange[ì œì£¼ ë§›ì§‘]ğŸ§‘ í›„íšŒëŠ” ì—†ì„ê±¸?!")

st.write("")

st.write("#í‘ë¼ì§€ #ì œì²  ìƒì„ íšŒ #í•´ë¬¼ë¼ë©´ #ìŠ¤í…Œì´í¬ #í•œì‹ #ì¤‘ì‹ #ì–‘ì‹ #ì¼ì‹ #í‘ë°±ìš”ë¦¬ì‚¬..ğŸ¤¤")

st.write("")

image_path = "https://pimg.mk.co.kr/news/cms/202409/22/news-p.v1.20240922.a626061476c54127bbe4beb0aa12d050_P1.png"
image_html = f"""
<div style="display: flex; justify-content: center;">
    <img src="{image_path}" alt="centered image" width="50%">
</div>
"""
st.markdown(image_html, unsafe_allow_html=True)

st.write("")

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ ì°¾ê³  ìˆë‚˜ìš”?"}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ ì°¾ê³  ìˆë‚˜ìš”?"}]
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)


# RAG

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# Hugging Faceì˜ ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "jhgan/ko-sroberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name).to(device)

print(f'Device is {device}.')


# FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index.index')):
    """
    FAISS ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.

    Parameters:
    index_path (str): ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ.

    Returns:
    faiss.Index: ë¡œë“œëœ FAISS ì¸ë±ìŠ¤ ê°ì²´.
    """
    if os.path.exists(index_path):
        # ì¸ë±ìŠ¤ íŒŒì¼ì—ì„œ ë¡œë“œ
        index = faiss.read_index(index_path)
        print(f"FAISS ì¸ë±ìŠ¤ê°€ {index_path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return index
    else:
        raise FileNotFoundError(f"{index_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# í…ìŠ¤íŠ¸ ì„ë² ë”©
def embed_text(text):
    # í† í¬ë‚˜ì´ì €ì˜ ì¶œë ¥ë„ GPUë¡œ ì´ë™
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
    with torch.no_grad():
        # ëª¨ë¸ì˜ ì¶œë ¥ì„ GPUì—ì„œ ì—°ì‚°í•˜ê³ , í•„ìš”í•œ ë¶€ë¶„ì„ ê°€ì ¸ì˜´
        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.squeeze().cpu().numpy()  # ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ê³  numpy ë°°ì—´ë¡œ ë³€í™˜

# ì„ë² ë”© ë¡œë“œ
embeddings = np.load(os.path.join(module_path, 'embeddings_array_file.npy'))

def generate_response_with_faiss(question, df, embeddings, model, embed_text, time, local_choice, index_path=os.path.join(module_path, 'faiss_index.index'), max_count=10, k=10, print_prompt=True):
    filtered_df = df

    # FAISS ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ
    index = load_faiss_index(index_path)

    # ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = embed_text(question).reshape(1, -1)

    # ê°€ì¥ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (3ë°°ìˆ˜)
    distances, indices = index.search(query_embedding, k*3)

    # FAISSë¡œ ê²€ìƒ‰ëœ ìƒìœ„ kê°œì˜ ë°ì´í„°í”„ë ˆì„ ì¶”ì¶œ
    filtered_df = filtered_df.iloc[indices[0, :]].copy().reset_index(drop=True)


    # ì›¹í˜ì´ì§€ì˜ ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒí•˜ëŠ” ì˜ì—…ì‹œê°„, í˜„ì§€ì¸ ë§›ì§‘ ì¡°ê±´ êµ¬í˜„

    # ì˜ì—…ì‹œê°„ ì˜µì…˜
    # í•„í„°ë§ ì¡°ê±´ìœ¼ë¡œ í™œìš©

    # ì˜ì—…ì‹œê°„ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°€ê²Œë“¤ë§Œ í•„í„°ë§
    if time == 'ìƒê´€ ì—†ìŒ':
        filtered_df = filtered_df
    elif time == 'ì•„ì¹¨':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(5, 12)))].reset_index(drop=True)
    elif time == 'ì ì‹¬':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(12, 14)))].reset_index(drop=True)
    elif time == 'ì˜¤í›„':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(14, 18)))].reset_index(drop=True)
    elif time == 'ì €ë…':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(18, 23)))].reset_index(drop=True)
    elif time == 'ë°¤':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in [23, 24, 1, 2, 3, 4]))].reset_index(drop=True)

    # í•„í„°ë§ í›„ ê°€ê²Œê°€ ì—†ìœ¼ë©´ ë©”ì‹œì§€ë¥¼ ë°˜í™˜
    if filtered_df.empty:
        return f"í˜„ì¬ ì„ íƒí•˜ì‹  ì‹œê°„ëŒ€({time})ì—ëŠ” ì˜ì—…í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    filtered_df = filtered_df.reset_index(drop=True)

    # í¬ë§ ê°€ê²©ëŒ€ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°€ê²Œë“¤ë§Œ í•„í„°ë§
    if price == 'ìƒê´€ ì—†ìŒ':
        filtered_df = filtered_df
    elif price == 'ìµœê³ ê°€':
        filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith('6')].reset_index(drop=True)
    elif price == 'ê³ ê°€':
        filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith('5'or'4')].reset_index(drop=True)
    elif price == 'í‰ê·  ê°€ê²©ëŒ€':
        filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith('3')].reset_index(drop=True)
    elif price == 'ì¤‘ì €ê°€':
        filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith('2')].reset_index(drop=True)
    elif price == 'ì €ê°€':
        filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith('1')].reset_index(drop=True)
 
    # í•„í„°ë§ í›„ ê°€ê²Œê°€ ì—†ìœ¼ë©´ ë°˜í™˜
    if filtered_df.empty:
        return f"í˜„ì¬ ì„ íƒí•˜ì‹  ì‹œê°„ëŒ€({time})ì—ëŠ” ì˜ì—…í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    filtered_df = filtered_df.reset_index(drop=True).head(k)


    # ì„ íƒëœ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì²˜ë¦¬
    if filtered_df.empty:
        return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."


    # ì°¸ê³ í•  ì •ë³´ì™€ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    reference_info = ""
    for idx, row in filtered_df.iterrows():
        reference_info += f"{row['text']}\n"

    # ì‘ë‹µì„ ë°›ì•„ì˜¤ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"ì§ˆë¬¸: {question} íŠ¹íˆ {price}ì„ ì„ í˜¸í•´\nì°¸ê³ í•  ì •ë³´:\n{reference_info}\nì‘ë‹µ:"

    if print_prompt:
        print('-----------------------------'*3)
        print(prompt)
        print('-----------------------------'*3)

    # ì‘ë‹µ ìƒì„±
    response = model.generate_content(prompt)

    return response


# User-provided prompt
if prompt := st.chat_input(): # (disabled=not replicate_api):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            # response = generate_llama2_response(prompt)
            response = generate_response_with_faiss(prompt, df, embeddings, model, embed_text, time, price)
            placeholder = st.empty()
            full_response = ''

            # ë§Œì•½ responseê°€ GenerateContentResponse ê°ì²´ë¼ë©´, ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
            if isinstance(response, str):
                full_response = response
            else:
                full_response = response.text  # response ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ë¶€ë¶„ ì¶”ì¶œ

            # for item in response:
            #     full_response += item
            #     placeholder.markdown(full_response)

            placeholder.markdown(full_response)
    message = {"role": "assistant", "content": full_response}
    st.session_state.messages.append(message)
