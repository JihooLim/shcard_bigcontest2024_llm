import os
import numpy as np
import pandas as pd

from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm
import faiss

import streamlit as st

# ê²½ë¡œ ì„¤ì •
data_path = './data'
module_path = './modules'

# Gemini ì„¤ì •
import google.generativeai as genai

GOOGLE_API_KEY = "AIzaSyBx1J1pS9k7bNA7R-5fkgAK8K7xQxd7Fes"

genai.configure(api_key=GOOGLE_API_KEY)

# Gemini ëª¨ë¸ ì„ íƒ
model = genai.GenerativeModel("gemini-1.5-flash")


# CSV íŒŒì¼ ë¡œë“œ
## ìì²´ ì „ì²˜ë¦¬ë¥¼ ê±°ì¹œ ë°ì´í„° íŒŒì¼ í™œìš©
csv_file_path = "JEJU_MCT_DATA_modified.csv"
df = pd.read_csv(os.path.join(data_path, csv_file_path))

# ìµœì‹ ì—°ì›” ë°ì´í„°ë§Œ ê°€ì ¸ì˜´
df = df[df['ê¸°ì¤€ì—°ì›”'] == df['ê¸°ì¤€ì—°ì›”'].max()].reset_index(drop=True)


# Streamlit App UI

st.set_page_config(page_title="ğŸŠì œì£¼ë„ê°€ ê·¸ë¦¬ ì¢‹ì•„!")

# Replicate Credentials
with st.sidebar:
    st.title("ğŸŠì œì£¼ë„ê°€ ê·¸ë¦¬ ì¢‹ì•„!")

    st.write("")

    st.subheader("ì‹œê°„ëŒ€ê°€ ì–´ë–»ê²Œ ë¼??")

    # selectbox ë ˆì´ë¸” ê³µë°± ì œê±°
    st.markdown(
        """
        <style>
        .stSelectbox label {  /* This targets the label element for selectbox */
            display: none;  /* Hides the label element */
        }
        .stSelectbox div[role='combobox'] {
            margin-top: -20px; /* Adjusts the margin if needed */
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    time = st.sidebar.selectbox("", ["ì•„ì¹¨", "ì ì‹¬", "ì˜¤í›„", "ì €ë…", "ë°¤"], key="time")

    st.write("")

    st.subheader("í˜„ì§€ì¸ ë§›ì§‘? ê´€ê´‘ê° ë§›ì§‘?")

    # radio ë ˆì´ë¸” ê³µë°± ì œê±°
    st.markdown(
        """
        <style>
        .stRadio > label {
            display: none;
        }
        .stRadio > div {
            margin-top: -20px;
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    local_choice = st.radio(
        '',
        ('ì œì£¼ë„ë¯¼ ë§›ì§‘', 'ê´€ê´‘ê° ë§›ì§‘')
    )

    st.write("")

st.title("ì–´ì„œì™€ìš©!ğŸ‘‹")
st.subheader("ì¸ê¸°ìˆëŠ” ì œì£¼ ë§›ì§‘ğŸ§‘â€ğŸ³ í›„íšŒëŠ” ì—†ì„ê±¸?!")

st.write("")

st.write("#í‘ë¼ì§€ #ì œì²  ìƒì„ íšŒ #í•´ë¬¼ë¼ë©´ #ìŠ¤í…Œì´í¬ #í•œì‹ #ì¤‘ì‹ #ì–‘ì‹ #ì¼ì‹ #í‘ë°±ìš”ë¦¬ì‚¬..ğŸ¤¤")

st.write("")

image_path = "https://pimg.mk.co.kr/news/cms/202409/22/news-p.v1.20240922.a626061476c54127bbe4beb0aa12d050_P1.png"
image_html = f"""
<div style="display: flex; justify-content: center;">
    <img src="{image_path}" alt="centered image" width="50%">
</div>
"""
st.markdown(image_html, unsafe_allow_html=True)

st.write("")

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}]
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)


# RAG

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# FAISS ì„¤ì •
model_name = "jhgan/ko-sroberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name).to("cpu")

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index.index')):
    if os.path.exists(index_path):
        index = faiss.read_index(index_path)
        return index
    else:
        raise FileNotFoundError(f"{index_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# í…ìŠ¤íŠ¸ ì„ë² ë”© í•¨ìˆ˜
def embed_text(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to("cpu")
    with torch.no_grad():
        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.squeeze().cpu().numpy()

# FAISS + Gemini ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response_with_priority(question, df, embeddings, model, embed_text, time, local_choice, index_path=os.path.join(module_path, 'faiss_index.index'), k=3):
    # 1. FAISS ì¸ë±ìŠ¤ë¥¼ í†µí•´ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ê²€ìƒ‰
    try:
        index = load_faiss_index(index_path)
    except:
        st.error(f"FAISS ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return None
    
    query_embedding = embed_text(question).reshape(1, -1)
    distances, indices = index.search(query_embedding, k*3)
    
    # ê²€ìƒ‰ëœ ìƒìœ„ kê°œì˜ ë°ì´í„° ì¶”ì¶œ
    filtered_df = df.iloc[indices[0, :]].copy().reset_index(drop=True)

    # ì˜ì—…ì‹œê°„ ì˜µì…˜
    if time == 'ì•„ì¹¨':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(5, 12)))].reset_index(drop=True)
    elif time == 'ì ì‹¬':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(12, 14)))].reset_index(drop=True)
    elif time == 'ì˜¤í›„':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(14, 18)))].reset_index(drop=True)
    elif time == 'ì €ë…':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(18, 23)))].reset_index(drop=True)
    elif time == 'ë°¤':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in [23, 24, 1, 2, 3, 4]))].reset_index(drop=True)

       # 2. ë°ì´í„°ê°€ ìˆì„ ê²½ìš°, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
    if not filtered_df.empty:
        response_text = "\n".join([row['text'] for _, row in filtered_df.iterrows()])
        return f"ë‹¤ìŒê³¼ ê°™ì€ ì¶”ì²œì´ ìˆìŠµë‹ˆë‹¤:\n{response_text}"

    # 3. ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°, Geminiì—ê²Œ ì§ˆë¬¸ì„ ë„˜ê²¨ì„œ ëŒ€ë‹µì„ ìƒì„±
    else:
        # Gemini ëª¨ë¸ì— ì§ˆë¬¸ì„ ì „ë‹¬í•˜ê³  ë‹µë³€ ìƒì„±
        prompt = f"ì§ˆë¬¸: {question} íŠ¹íˆ {local_choice}ì„ ì„ í˜¸í•´"
        response = model.generate_content(prompt)
        except:
            st.error(f"Geminiì—ì„œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None
        return response


# ì„ë² ë”© ë¡œë“œ í™•ì¸
embeddings_path = os.path.join(module_path, 'embeddings_array_file.npy')
if os.path.exists(embeddings_path):
    embeddings = np.load(embeddings_path)
else:
    st.error(f"{embeddings_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    embeddings = None

# ìœ ì € ì§ˆë¬¸ ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # Generate response with priority (FAISS -> Gemini)
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                response = generate_response_with_priority(prompt, df, embeddings, model, embed_text, time, local_choice)
                st.write(response)
                st.session_state.messages.append({"role": "assistant", "content": response})

   
